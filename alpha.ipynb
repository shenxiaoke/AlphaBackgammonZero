{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecc5625-b963-4e79-955b-8a994b564412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shenke/miniconda3/envs/pytorch_gpu/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "835756b9-29e3-4264-8e7f-94d42de3ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(Box) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d05a5162-1709-42b6-a3d0-4be6ed6ca2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ada603-5849-4e33-a7e8-6c01c94f0e95",
   "metadata": {
    "tags": []
   },
   "source": [
    "## params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d697e430-fd90-47f6-b1bd-cd1034b1823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_width = 6        # 棋盘宽\n",
    "board_height = 6       # 棋盘高\n",
    "n_in_row = 4           # 胜利需要连成线棋子\n",
    "c_puct = 5             # 决定探索程度\n",
    "n_playout = 100        # 每步模拟次数\n",
    "learn_rate = 0.002     # 学习率\n",
    "lr_multiplier = 1.0    # 基于KL的自适应学习率调整\n",
    "temperature = 1.0      # 温度参数\n",
    "noise_eps = 0.75       # 噪声参数\n",
    "dirichlet_alpha = 0.3  # dirichlet系数\n",
    "buffer_size = 5000     # buffer大小\n",
    "train_batch_size = 128 # batchsize大小\n",
    "update_epochs = 5      # 多少个epoch更新一次\n",
    "kl_coeff = 0.02        # kl系数\n",
    "checkpoint_freq = 20   # 模型保存频率\n",
    "mcts_infer = 200       # 纯mcts推理时间\n",
    "restore_model = None   # 是否加载预训练模型\n",
    "game_batch_num=40      # 训练步数\n",
    "model_path=\".\"         # 模型保存路径"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21836703-5bde-462f-b2f0-cd74def41ba7",
   "metadata": {},
   "source": [
    "## create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2094368a-75d4-40dc-8fd4-74a52b0e0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GomokuEnv(gym.Env):\n",
    "    def __init__(self, start_player=0):\n",
    "        self.start_player = start_player\n",
    "\n",
    "        self.action_space = Discrete((board_width * board_height))\n",
    "        self.observation_space = Box(0, 1, shape=(4, board_width, board_height))\n",
    "        self.reward = 0\n",
    "        self.info = {}\n",
    "        self.players = [1, 2]  # player1 and player2\n",
    "\n",
    "    def step(self, action):\n",
    "        self.states[action] = self.current_player\n",
    "        if action in self.availables:\n",
    "            self.availables.remove(action)\n",
    "\n",
    "        self.last_move = action\n",
    "\n",
    "        done, winner = self.game_end()\n",
    "        reward = 0\n",
    "        if done:\n",
    "            if winner == self.current_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "\n",
    "        self.current_player = (\n",
    "            self.players[0] if self.current_player == self.players[1]\n",
    "            else self.players[1]\n",
    "        )\n",
    "\n",
    "        # update state\n",
    "        obs = self.current_state()\n",
    "\n",
    "        return obs, reward, done, self.info\n",
    "\n",
    "    def reset(self):\n",
    "        if board_width < n_in_row or board_height < n_in_row:\n",
    "            raise Exception('board width and height can not be '\n",
    "                            'less than {}'.format(n_in_row))\n",
    "        self.current_player = self.players[self.start_player]  # start player\n",
    "        # keep available moves in a list\n",
    "        self.availables = list(range(board_width * board_height))\n",
    "        self.states = {}\n",
    "        self.last_move = -1\n",
    "\n",
    "        return self.current_state()\n",
    "\n",
    "    def render(self, mode='human', start_player=0):\n",
    "        width = board_width\n",
    "        height = board_height\n",
    "\n",
    "        p1, p2 = self.players\n",
    "\n",
    "        print()\n",
    "        for x in range(width):\n",
    "            print(\"{0:8}\".format(x), end='')\n",
    "        print('\\r\\n')\n",
    "        for i in range(height - 1, -1, -1):\n",
    "            print(\"{0:4d}\".format(i), end='')\n",
    "            for j in range(width):\n",
    "                loc = i * width + j\n",
    "                p = self.states.get(loc, -1)\n",
    "                if p == p1:\n",
    "                    print('B'.center(8), end='')\n",
    "                elif p == p2:\n",
    "                    print('W'.center(8), end='')\n",
    "                else:\n",
    "                    print('_'.center(8), end='')\n",
    "            print('\\r\\n\\r\\n')\n",
    "\n",
    "    def has_a_winner(self):\n",
    "        states = self.states\n",
    "        moved = list(set(range(board_width * board_height)) - set(self.availables))\n",
    "        if len(moved) < n_in_row * 2 - 1:\n",
    "            return False, -1\n",
    "\n",
    "        for m in moved:\n",
    "            h = m // board_width\n",
    "            w = m % board_width\n",
    "            player = states[m]\n",
    "\n",
    "            if (w in range(board_width - n_in_row + 1) and\n",
    "                    len(set(states.get(i, -1) for i in range(m, m + n_in_row))) == 1):\n",
    "                return True, player\n",
    "\n",
    "            if (h in range(board_height - n_in_row + 1) and\n",
    "                    len(set(states.get(i, -1) for i in range(m, m + n_in_row * board_width, board_width))) == 1):\n",
    "                return True, player\n",
    "\n",
    "            if (w in range(board_width - n_in_row + 1) and h in range(board_height - n_in_row + 1) and\n",
    "                    len(set(\n",
    "                        states.get(i, -1) for i in range(m, m + n_in_row * (board_width + 1), board_width + 1))) == 1):\n",
    "                return True, player\n",
    "\n",
    "            if (w in range(n_in_row - 1, board_width) and h in range(board_height - n_in_row + 1) and\n",
    "                    len(set(\n",
    "                        states.get(i, -1) for i in range(m, m + n_in_row * (board_width - 1), board_width - 1))) == 1):\n",
    "                return True, player\n",
    "\n",
    "        return False, -1\n",
    "\n",
    "    def game_end(self):\n",
    "        \"\"\"Check whether the game is ended or not\"\"\"\n",
    "        win, winner = self.has_a_winner()\n",
    "        if win:\n",
    "            # print(\"winner is player{}\".format(winner))\n",
    "            return True, winner\n",
    "        elif not len(self.availables):\n",
    "            return True, -1\n",
    "        return False, -1\n",
    "\n",
    "    def current_state(self):\n",
    "        \"\"\"return the board state from the perspective of the current player.\n",
    "        state shape: 4*width*height\n",
    "        \"\"\"\n",
    "        square_state = np.zeros((4, board_width, board_height))\n",
    "        if self.states:\n",
    "            moves, players = np.array(list(zip(*self.states.items())))\n",
    "            move_curr = moves[players == self.current_player]\n",
    "            move_oppo = moves[players != self.current_player]\n",
    "            square_state[0][move_curr // board_width,\n",
    "                            move_curr % board_height] = 1.0\n",
    "            square_state[1][move_oppo // board_width,\n",
    "                            move_oppo % board_height] = 1.0\n",
    "            # indicate the last move location\n",
    "            square_state[2][self.last_move // board_width,\n",
    "                            self.last_move % board_height] = 1.0\n",
    "        if len(self.states) % 2 == 0:\n",
    "            square_state[3][:, :] = 1.0  # indicate the colour to play\n",
    "        return square_state[:, ::-1, :]\n",
    "\n",
    "    def start_play(self, player1, player2, start_player=0):\n",
    "        \"\"\"start a game between two players\"\"\"\n",
    "        if start_player not in (0, 1):\n",
    "            raise Exception('start_player should be either 0 (player1 first) '\n",
    "                            'or 1 (player2 first)')\n",
    "        self.reset()\n",
    "        p1, p2 = self.players\n",
    "        player1.set_player_ind(p1)\n",
    "        player2.set_player_ind(p2)\n",
    "        players = {p1: player1, p2: player2}\n",
    "        while True:\n",
    "            player_in_turn = players[self.current_player]\n",
    "            move = player_in_turn.get_action(self)\n",
    "            self.step(move)\n",
    "            end, winner = self.game_end()\n",
    "            if end:\n",
    "                return winner\n",
    "\n",
    "    def start_self_play(self, player):\n",
    "        \"\"\" start a self-play game using a MCTS player, reuse the search tree,\n",
    "        and store the self-play data: (state, mcts_probs, z) for training\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        states, mcts_probs, current_players = [], [], []\n",
    "        while True:\n",
    "            move, move_probs = player.get_action(self, return_prob=1)\n",
    "            # store the data\n",
    "            states.append(self.current_state())\n",
    "            mcts_probs.append(move_probs)\n",
    "            current_players.append(self.current_player)\n",
    "            # perform a move\n",
    "            self.step(move)\n",
    "            end, winner = self.game_end()\n",
    "            if end:\n",
    "                # winner from the perspective of the current player of each state\n",
    "                winners_z = np.zeros(len(current_players))\n",
    "                if winner != -1:\n",
    "                    winners_z[np.array(current_players) == winner] = 1.0\n",
    "                    winners_z[np.array(current_players) != winner] = -1.0\n",
    "                # reset MCTS root node\n",
    "                player.reset_player()\n",
    "                return winner, zip(states, mcts_probs, winners_z)\n",
    "            \n",
    "    def location_to_move(self, location):\n",
    "        if (len(location) != 2):\n",
    "            return -1\n",
    "        h = location[0]\n",
    "        w = location[1]\n",
    "        move = h * board_width + w\n",
    "        if (move not in range(board_width * board_width)):\n",
    "            return -1\n",
    "        return move\n",
    "\n",
    "    def move_to_location(self, move):\n",
    "        \"\"\"\n",
    "        3*3 board's moves like:\n",
    "        6 7 8\n",
    "        3 4 5\n",
    "        0 1 2\n",
    "        and move 5's location is (1,2)\n",
    "        \"\"\"\n",
    "        h = move // board_width\n",
    "        w = move % board_width\n",
    "        return [h, w]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78cd817-b990-46f5-b622-f8d587b2e234",
   "metadata": {},
   "source": [
    "## create net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf6b24f5-cf2c-4575-a88a-ff21e51ac6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"policy-value network module\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # common layers\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        # action policy layers\n",
    "        self.act_conv1 = nn.Conv2d(128, 4, kernel_size=1)\n",
    "        self.act_fc1 = nn.Linear(4 * board_width * board_height,\n",
    "                                 board_width * board_height)\n",
    "        # state value layers\n",
    "        self.val_conv1 = nn.Conv2d(128, 2, kernel_size=1)\n",
    "        self.val_fc1 = nn.Linear(2 * board_width * board_height, 64)\n",
    "        self.val_fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state_input):\n",
    "        # common layers\n",
    "        x = F.relu(self.conv1(state_input))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # action policy layers\n",
    "        x_act = F.relu(self.act_conv1(x))\n",
    "        x_act = x_act.view(-1, 4 * board_width * board_height)\n",
    "        x_act = F.log_softmax(self.act_fc1(x_act))\n",
    "        # state value layers\n",
    "        x_val = F.relu(self.val_conv1(x))\n",
    "        x_val = x_val.view(-1, 2 * board_width * board_height)\n",
    "        x_val = F.relu(self.val_fc1(x_val))\n",
    "        x_val = F.tanh(self.val_fc2(x_val))\n",
    "\n",
    "        return x_act, x_val\n",
    "\n",
    "\n",
    "class PolicyValueNet:\n",
    "    \"\"\"policy-value network \"\"\"\n",
    "\n",
    "    def __init__(self, model_file=None):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.l2_const = 1e-4  # coef of l2 penalty\n",
    "        # the policy value net module\n",
    "\n",
    "        self.policy_value_net = Net().to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy_value_net.parameters(),\n",
    "                                    weight_decay=self.l2_const)\n",
    "\n",
    "        if model_file:\n",
    "            net_params = torch.load(model_file)\n",
    "            self.policy_value_net.load_state_dict(net_params)\n",
    "\n",
    "    def policy_value(self, state_batch):\n",
    "        \"\"\"\n",
    "        input: a batch of states\n",
    "        output: a batch of action probabilities and state values\n",
    "        \"\"\"\n",
    "        state_batch = Variable(torch.FloatTensor(state_batch).to(self.device))\n",
    "        log_act_probs, value = self.policy_value_net(state_batch)\n",
    "        act_probs = np.exp(log_act_probs.data.cpu().numpy())\n",
    "        return act_probs, value.data.cpu().numpy()\n",
    "\n",
    "    def policy_value_fn(self, board):\n",
    "        \"\"\"\n",
    "        input: board\n",
    "        output: a list of (action, probability) tuples for each available\n",
    "        action and the score of the board state\n",
    "        \"\"\"\n",
    "        legal_positions = board.availables\n",
    "        current_state = np.ascontiguousarray(board.current_state().reshape(\n",
    "            -1, 4, board_width, board_height))\n",
    "        log_act_probs, value = self.policy_value_net(\n",
    "            Variable(torch.from_numpy(current_state)).to(self.device).float())\n",
    "        act_probs = np.exp(log_act_probs.data.cpu().numpy().flatten())\n",
    "        act_probs = zip(legal_positions, act_probs[legal_positions])\n",
    "        value = value.data[0][0]\n",
    "        return act_probs, value\n",
    "\n",
    "    def train_step(self, state_batch, mcts_probs, winner_batch, lr):\n",
    "        \"\"\"perform a training step\"\"\"\n",
    "        # wrap in Variable\n",
    "        state_batch = Variable(torch.FloatTensor(state_batch).to(self.device))\n",
    "        mcts_probs = Variable(torch.FloatTensor(mcts_probs).to(self.device))\n",
    "        winner_batch = Variable(torch.FloatTensor(winner_batch).to(self.device))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        # set learning rate\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # forward\n",
    "        log_act_probs, value = self.policy_value_net(state_batch)\n",
    "        # define the loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n",
    "        # Note: the L2 penalty is incorporated in optimizer\n",
    "        value_loss = F.mse_loss(value.view(-1), winner_batch)\n",
    "        policy_loss = -torch.mean(torch.sum(mcts_probs * log_act_probs, 1))\n",
    "        loss = value_loss + policy_loss\n",
    "        # backward and optimize\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # calc policy entropy, for monitoring only\n",
    "        entropy = -torch.mean(\n",
    "            torch.sum(torch.exp(log_act_probs) * log_act_probs, 1)\n",
    "        )\n",
    "        # return loss.data, entropy.data\n",
    "        # for pytorch version >= 0.5 please use the following line instead.\n",
    "        return loss.item(), entropy.item()\n",
    "\n",
    "    def get_policy_param(self):\n",
    "        net_params = self.policy_value_net.state_dict()\n",
    "        return net_params\n",
    "\n",
    "    def save_model(self, model_file):\n",
    "        \"\"\" save model params to file \"\"\"\n",
    "        net_params = self.get_policy_param()  # get model params\n",
    "        torch.save(net_params, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd911036-acdc-486e-85b6-c6151acc6bf6",
   "metadata": {},
   "source": [
    "## implement MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b8aebf8-8ea0-496a-b1e4-4390dcbadc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    probs = np.exp(x - np.max(x))\n",
    "    probs /= np.sum(probs)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def rollout_policy_fn(board):\n",
    "    \"\"\"a coarse, fast version of policy_fn used in the rollout phase.\"\"\"\n",
    "    # rollout randomly\n",
    "    action_probs = np.random.rand(len(board.availables))\n",
    "    return zip(board.availables, action_probs)\n",
    "\n",
    "\n",
    "def policy_value_fn(board):\n",
    "    \"\"\"a function that takes in a state and outputs a list of (action, probability)\n",
    "    tuples and a score for the state\"\"\"\n",
    "    # return uniform probabilities and 0 score for pure MCTS\n",
    "    action_probs = np.ones(len(board.availables)) / len(board.availables)\n",
    "    return zip(board.availables, action_probs), 0\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    \"\"\"A node in the MCTS tree.\n",
    "\n",
    "    Each node keeps track of its own value Q, prior probability P, and\n",
    "    its visit-count-adjusted prior score u.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parent, prior_p):\n",
    "        self._parent = parent\n",
    "        self._children = {}  # a map from action to TreeNode\n",
    "        self._n_visits = 0\n",
    "        self._Q = 0\n",
    "        self._u = 0\n",
    "        self._P = prior_p\n",
    "\n",
    "    def expand(self, action_priors):\n",
    "        \"\"\"Expand tree by creating new children.\n",
    "        action_priors: a list of tuples of actions and their prior probability\n",
    "            according to the policy function.\n",
    "        \"\"\"\n",
    "        for action, prob in action_priors:\n",
    "            if action not in self._children:\n",
    "                self._children[action] = TreeNode(self, prob)\n",
    "\n",
    "    def select(self, c_puct):\n",
    "        \"\"\"Select action among children that gives maximum action value Q\n",
    "        plus bonus u(P).\n",
    "        Return: A tuple of (action, next_node)\n",
    "        \"\"\"\n",
    "        return max(self._children.items(),\n",
    "                   key=lambda act_node: act_node[1].get_value(c_puct))\n",
    "\n",
    "    def update(self, leaf_value):\n",
    "        \"\"\"Update node values from leaf evaluation.\n",
    "        leaf_value: the value of subtree evaluation from the current player's\n",
    "            perspective.\n",
    "        \"\"\"\n",
    "        # Count visit.\n",
    "        self._n_visits += 1\n",
    "        # Update Q, a running average of values for all visits.\n",
    "        self._Q += 1.0 * (leaf_value - self._Q) / self._n_visits\n",
    "\n",
    "    def update_recursive(self, leaf_value):\n",
    "        \"\"\"Like a call to update(), but applied recursively for all ancestors.\n",
    "        \"\"\"\n",
    "        # If it is not root, this node's parent should be updated first.\n",
    "        if self._parent:\n",
    "            self._parent.update_recursive(-leaf_value)\n",
    "        self.update(leaf_value)\n",
    "\n",
    "    def get_value(self, c_puct):\n",
    "        \"\"\"Calculate and return the value for this node.\n",
    "        It is a combination of leaf evaluations Q, and this node's prior\n",
    "        adjusted for its visit count, u.\n",
    "        c_puct: a number in (0, inf) controlling the relative impact of\n",
    "            value Q, and prior probability P, on this node's score.\n",
    "        \"\"\"\n",
    "        self._u = (c_puct * self._P *\n",
    "                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\n",
    "        return self._Q + self._u\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"Check if leaf node (i.e. no nodes below this have been expanded).\"\"\"\n",
    "        return self._children == {}\n",
    "\n",
    "    def is_root(self):\n",
    "        return self._parent is None\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"An implementation of Monte Carlo Tree Search.\"\"\"\n",
    "\n",
    "    def __init__(self, policy_value_fn, c_puct=5):\n",
    "        \"\"\"\n",
    "        policy_value_fn: a function that takes in a board state and outputs\n",
    "            a list of (action, probability) tuples and also a score in [-1, 1]\n",
    "            (i.e. the expected value of the end game score from the current\n",
    "            player's perspective) for the current player.\n",
    "        c_puct: a number in (0, inf) that controls how quickly exploration\n",
    "            converges to the maximum-value policy. A higher value means\n",
    "            relying on the prior more.\n",
    "        \"\"\"\n",
    "        self._root = TreeNode(None, 1.0)\n",
    "        self._policy = policy_value_fn\n",
    "        self._c_puct = c_puct\n",
    "\n",
    "    def _playout(self, state):\n",
    "        \"\"\"Run a single playout from the root to the leaf, getting a value at\n",
    "        the leaf and propagating it back through its parents.\n",
    "        State is modified in-place, so a copy must be provided.\n",
    "        \"\"\"\n",
    "        node = self._root\n",
    "        while (1):\n",
    "            if node.is_leaf():\n",
    "                break\n",
    "            # Greedily select next move.\n",
    "            action, node = node.select(self._c_puct)\n",
    "            state.step(action)\n",
    "\n",
    "        # Evaluate the leaf using a network which outputs a list of\n",
    "        # (action, probability) tuples p and also a score v in [-1, 1]\n",
    "        # for the current player.\n",
    "        action_probs, leaf_value = self._policy(state)\n",
    "        # Check for end of game.\n",
    "        end, winner = state.game_end()\n",
    "        if not end:\n",
    "            node.expand(action_probs)\n",
    "        else:\n",
    "            # for end state, return the true leaf_value\n",
    "            if winner == -1:  # tie\n",
    "                leaf_value = 0.0\n",
    "            else:\n",
    "                leaf_value = (\n",
    "                    1.0 if winner == state.current_player else -1.0\n",
    "                )\n",
    "\n",
    "        # Update value and visit count of nodes in this traversal.\n",
    "        node.update_recursive(-leaf_value)\n",
    "\n",
    "    def _playout_p(self, state):\n",
    "        \"\"\"Run a single playout from the root to the leaf, getting a value at\n",
    "        the leaf and propagating it back through its parents.\n",
    "        State is modified in-place, so a copy must be provided.\n",
    "        \"\"\"\n",
    "        node = self._root\n",
    "        while (1):\n",
    "            if node.is_leaf():\n",
    "                break\n",
    "            # Greedily select next move.\n",
    "            action, node = node.select(self._c_puct)\n",
    "            state.step(action)\n",
    "\n",
    "        action_probs, _ = self._policy(state)\n",
    "        # Check for end of game\n",
    "        end, winner = state.game_end()\n",
    "        if not end:\n",
    "            node.expand(action_probs)\n",
    "        # Evaluate the leaf node by random rollout\n",
    "        leaf_value = self._evaluate_rollout(state)\n",
    "        # Update value and visit count of nodes in this traversal.\n",
    "        node.update_recursive(-leaf_value)\n",
    "\n",
    "    def _evaluate_rollout(self, env, limit=1000):\n",
    "        \"\"\"Use the rollout policy to play until the end of the game,\n",
    "        returning +1 if the current player wins, -1 if the opponent wins,\n",
    "        and 0 if it is a tie.\n",
    "        \"\"\"\n",
    "        player = env.current_player\n",
    "        for i in range(limit):\n",
    "            end, winner = env.game_end()\n",
    "            if end:\n",
    "                break\n",
    "            action_probs = rollout_policy_fn(env)\n",
    "            max_action = max(action_probs, key=itemgetter(1))[0]\n",
    "            env.step(max_action)\n",
    "        else:\n",
    "            # If no break from the loop, issue a warning.\n",
    "            print(\"WARNING: rollout reached move limit\")\n",
    "        if winner == -1:  # tie\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 if winner == player else -1\n",
    "\n",
    "    def get_move_probs(self, state, temp=1e-3):\n",
    "        \"\"\"Run all playouts sequentially and return the available actions and\n",
    "        their corresponding probabilities.\n",
    "        state: the current game state\n",
    "        temp: temperature parameter in (0, 1] controls the level of exploration\n",
    "        \"\"\"\n",
    "        for n in range(n_playout):\n",
    "            state_copy = copy.deepcopy(state)\n",
    "            self._playout(state_copy)\n",
    "\n",
    "        # calc the move probabilities based on visit counts at the root node\n",
    "        act_visits = [(act, node._n_visits)\n",
    "                      for act, node in self._root._children.items()]\n",
    "        acts, visits = zip(*act_visits)\n",
    "        act_probs = softmax(1.0 / temp * np.log(np.array(visits) + 1e-10))\n",
    "\n",
    "        return acts, act_probs\n",
    "\n",
    "    def get_move(self, state):\n",
    "        \"\"\"Runs all playouts sequentially and returns the most visited action.\n",
    "        state: the current game state\n",
    "\n",
    "        Return: the selected action\n",
    "        \"\"\"\n",
    "        for n in range(n_playout):\n",
    "            state_copy = copy.deepcopy(state)\n",
    "            self._playout_p(state_copy)\n",
    "        return max(self._root._children.items(),\n",
    "                   key=lambda act_node: act_node[1]._n_visits)[0]\n",
    "\n",
    "    def update_with_move(self, last_move):\n",
    "        \"\"\"Step forward in the tree, keeping everything we already know\n",
    "        about the subtree.\n",
    "        \"\"\"\n",
    "        if last_move in self._root._children:\n",
    "            self._root = self._root._children[last_move]\n",
    "            self._root._parent = None\n",
    "        else:\n",
    "            self._root = TreeNode(None, 1.0)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MCTS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb22e985-0b7e-436f-b887-953c51ac6788",
   "metadata": {},
   "source": [
    "## self-play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47049085-0a45-42c7-85e1-77af8d05c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS_Pure:\n",
    "    \"\"\"AI player based on MCTS\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mcts = MCTS(policy_value_fn, c_puct)\n",
    "\n",
    "    def set_player_ind(self, p):\n",
    "        self.player = p\n",
    "\n",
    "    def reset_player(self):\n",
    "        self.mcts.update_with_move(-1)\n",
    "\n",
    "    def get_action(self, board):\n",
    "        sensible_moves = board.availables\n",
    "        if len(sensible_moves) > 0:\n",
    "            move = self.mcts.get_move(board)\n",
    "            self.mcts.update_with_move(-1)\n",
    "            return move\n",
    "        else:\n",
    "            print(\"WARNING: the board is full\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MCTS {}\".format(self.player)\n",
    "\n",
    "\n",
    "class MCTSPlayer(MCTS_Pure):\n",
    "    \"\"\"AI player based on MCTS\"\"\"\n",
    "\n",
    "    def __init__(self, policy_value_function, is_selfplay=0):\n",
    "        super(MCTS_Pure, self).__init__()\n",
    "        self.mcts = MCTS(policy_value_function, c_puct)\n",
    "        self._is_selfplay = is_selfplay\n",
    "\n",
    "    def get_action(self, env, return_prob=0):\n",
    "        sensible_moves = env.availables\n",
    "        # the pi vector returned by MCTS as in the alphaGo Zero paper\n",
    "        move_probs = np.zeros(board_width * board_width)\n",
    "        if len(sensible_moves) > 0:\n",
    "            acts, probs = self.mcts.get_move_probs(env, temperature)\n",
    "            move_probs[list(acts)] = probs\n",
    "            if self._is_selfplay:\n",
    "                # add Dirichlet Noise for exploration (needed for\n",
    "                # self-play training)\n",
    "                move = np.random.choice(\n",
    "                    acts,\n",
    "                    p=noise_eps * probs + (1 - noise_eps) * np.random.dirichlet(\n",
    "                        dirichlet_alpha * np.ones(len(probs))))\n",
    "                # update the root node and reuse the search tree\n",
    "                self.mcts.update_with_move(move)\n",
    "            else:\n",
    "                # with the default temp=1e-3, it is almost equivalent\n",
    "                # to choosing the move with the highest prob\n",
    "                move = np.random.choice(acts, p=probs)\n",
    "                # reset the root node\n",
    "                self.mcts.update_with_move(-1)\n",
    "\n",
    "            if return_prob:\n",
    "                return move, move_probs\n",
    "            else:\n",
    "                return move\n",
    "        else:\n",
    "            print(\"WARNING: the board is full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770ac26-01dd-4fd1-9f13-eb3736204455",
   "metadata": {},
   "source": [
    "## train main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ec9d75a-c32e-4056-9c43-6a77bb776fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainPipeline:\n",
    "    def __init__(self):\n",
    "        # params of the board and the game\n",
    "        self.env = GomokuEnv()\n",
    "\n",
    "        # training params\n",
    "        self.data_buffer = deque(maxlen=buffer_size)\n",
    "        self.play_batch_size = 1\n",
    "        self.best_win_ratio = 0.0\n",
    "\n",
    "        # start training from an initial policy-value net\n",
    "        self.policy_value_net = PolicyValueNet(model_file=restore_model)\n",
    "\n",
    "        self.mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
    "                                      is_selfplay=1)\n",
    "        self.mcts_infer = mcts_infer\n",
    "        self.lr_multiplier = lr_multiplier\n",
    "\n",
    "    def get_equi_data(self, play_data):\n",
    "        \"\"\"augment the data set by rotation and flipping\n",
    "        play_data: [(state, mcts_prob, winner_z), ..., ...]\n",
    "        \"\"\"\n",
    "        extend_data = []\n",
    "        for state, mcts_porb, winner in play_data:\n",
    "            for i in [1, 2, 3, 4]:\n",
    "                # rotate counterclockwise\n",
    "                equi_state = np.array([np.rot90(s, i) for s in state])\n",
    "                equi_mcts_prob = np.rot90(np.flipud(\n",
    "                    mcts_porb.reshape(board_height, board_width)), i)\n",
    "                extend_data.append((equi_state,\n",
    "                                    np.flipud(equi_mcts_prob).flatten(),\n",
    "                                    winner))\n",
    "                # flip horizontally\n",
    "                equi_state = np.array([np.fliplr(s) for s in equi_state])\n",
    "                equi_mcts_prob = np.fliplr(equi_mcts_prob)\n",
    "                extend_data.append((equi_state,\n",
    "                                    np.flipud(equi_mcts_prob).flatten(),\n",
    "                                    winner))\n",
    "        return extend_data\n",
    "\n",
    "    def collect_selfplay_data(self, n_games=1):\n",
    "        \"\"\"collect self-play data for training\"\"\"\n",
    "        for i in range(n_games):\n",
    "            winner, play_data = self.env.start_self_play(self.mcts_player)\n",
    "            play_data = list(play_data)[:]\n",
    "            self.episode_len = len(play_data)\n",
    "            # augment the data\n",
    "            play_data = self.get_equi_data(play_data)\n",
    "            self.data_buffer.extend(play_data)\n",
    "\n",
    "    def policy_update(self):\n",
    "        \"\"\"update the policy-value net\"\"\"\n",
    "        mini_batch = random.sample(self.data_buffer, train_batch_size)\n",
    "        state_batch = [data[0] for data in mini_batch]\n",
    "        mcts_probs_batch = [data[1] for data in mini_batch]\n",
    "        winner_batch = [data[2] for data in mini_batch]\n",
    "        old_probs, old_v = self.policy_value_net.policy_value(state_batch)\n",
    "        for i in range(update_epochs):\n",
    "            loss, entropy = self.policy_value_net.train_step(\n",
    "                state_batch,\n",
    "                mcts_probs_batch,\n",
    "                winner_batch,\n",
    "                learn_rate * self.lr_multiplier)\n",
    "            new_probs, new_v = self.policy_value_net.policy_value(state_batch)\n",
    "            kl = np.mean(np.sum(old_probs * (\n",
    "                    np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)),\n",
    "                                axis=1)\n",
    "                         )\n",
    "            if kl > kl_coeff * 4:  # early stopping if D_KL diverges badly\n",
    "                break\n",
    "        # adaptively adjust the learning rate\n",
    "        if kl > kl_coeff * 2 and self.lr_multiplier > 0.1:\n",
    "            self.lr_multiplier /= 1.5\n",
    "        elif kl < kl_coeff / 2 and self.lr_multiplier < 10:\n",
    "            self.lr_multiplier *= 1.5\n",
    "        return loss, entropy\n",
    "\n",
    "    def policy_evaluate(self, n_games=10):\n",
    "        \"\"\"\n",
    "        Evaluate the trained policy by playing against the pure MCTS player\n",
    "        Note: this is only for monitoring the progress of training\n",
    "        \"\"\"\n",
    "        current_mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn)\n",
    "        pure_mcts_player = MCTS_Pure()\n",
    "        win_cnt = defaultdict(int)\n",
    "        for i in range(n_games):\n",
    "            winner = self.env.start_play(current_mcts_player,\n",
    "                                         pure_mcts_player,\n",
    "                                         start_player=i % 2)\n",
    "            win_cnt[winner] += 1\n",
    "        win_ratio = 1.0 * (win_cnt[1] + 0.5 * win_cnt[-1]) / n_games\n",
    "        print(\"num_playouts:{}, win: {}, lose: {}, tie:{}\".format(self.mcts_infer,\n",
    "                                                                  win_cnt[1], win_cnt[2], win_cnt[-1]))\n",
    "        return win_ratio\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"run the training pipeline\"\"\"\n",
    "        win_num = 0\n",
    "        try:\n",
    "            for i_step in range(game_batch_num):\n",
    "                self.collect_selfplay_data(self.play_batch_size)\n",
    "                print(\"batch i:{}, episode_len:{}\".format(\n",
    "                    i_step + 1, self.episode_len))\n",
    "                if len(self.data_buffer) > train_batch_size:\n",
    "                    loss, entropy = self.policy_update()\n",
    "                # check the performance of the current model,\n",
    "                # and save the model params\n",
    "                if (i_step + 1) % checkpoint_freq == 0:\n",
    "                    print(\"current self-play batch: {}\".format(i_step + 1))\n",
    "                    win_ratio = self.policy_evaluate()\n",
    "                    self.policy_value_net.save_model(os.path.join(model_path, \"newest_model.pt\"))\n",
    "                    if win_ratio > self.best_win_ratio:\n",
    "                        win_num += 1\n",
    "                        # print(\"New best policy!!!!!!!!\")\n",
    "                        self.best_win_ratio = win_ratio\n",
    "                        # update the best_policy\n",
    "                        self.policy_value_net.save_model(os.path.join(model_path, \"best_model.pt\"))\n",
    "                        if self.best_win_ratio == 1.0 and self.mcts_infer < 5000:\n",
    "                            self.mcts_infer += 1000\n",
    "                            self.best_win_ratio = 0.0\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\n\\rquit')\n",
    "\n",
    "        return win_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426848ab-ff67-40d6-96d8-5e8586609a1c",
   "metadata": {},
   "source": [
    "## self-play train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f7e62f1-6339-470c-9e37-1a52300dedc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4v/_ggyyph946s3tv60n6fng92w0000gn/T/ipykernel_731/3097606431.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x_act = F.log_softmax(self.act_fc1(x_act))\n",
      "/Users/shenke/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch i:1, episode_len:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4v/_ggyyph946s3tv60n6fng92w0000gn/T/ipykernel_731/3097606431.py:64: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1670076272873/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  state_batch = Variable(torch.FloatTensor(state_batch).to(self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch i:2, episode_len:10\n",
      "batch i:3, episode_len:14\n",
      "batch i:4, episode_len:12\n",
      "batch i:5, episode_len:14\n",
      "batch i:6, episode_len:10\n",
      "batch i:7, episode_len:12\n",
      "batch i:8, episode_len:12\n",
      "batch i:9, episode_len:23\n",
      "batch i:10, episode_len:15\n",
      "batch i:11, episode_len:13\n",
      "batch i:12, episode_len:14\n",
      "batch i:13, episode_len:12\n",
      "batch i:14, episode_len:14\n",
      "batch i:15, episode_len:12\n",
      "batch i:16, episode_len:23\n",
      "batch i:17, episode_len:14\n",
      "batch i:18, episode_len:17\n",
      "batch i:19, episode_len:14\n",
      "batch i:20, episode_len:14\n",
      "current self-play batch: 20\n",
      "num_playouts:200, win: 2, lose: 8, tie:0\n",
      "batch i:21, episode_len:20\n",
      "batch i:22, episode_len:22\n",
      "batch i:23, episode_len:15\n",
      "batch i:24, episode_len:9\n",
      "batch i:25, episode_len:19\n",
      "batch i:26, episode_len:13\n",
      "batch i:27, episode_len:10\n",
      "batch i:28, episode_len:21\n",
      "batch i:29, episode_len:18\n",
      "batch i:30, episode_len:19\n",
      "batch i:31, episode_len:17\n",
      "batch i:32, episode_len:19\n",
      "batch i:33, episode_len:13\n",
      "batch i:34, episode_len:17\n",
      "batch i:35, episode_len:15\n",
      "batch i:36, episode_len:16\n",
      "batch i:37, episode_len:19\n",
      "batch i:38, episode_len:17\n",
      "batch i:39, episode_len:21\n",
      "batch i:40, episode_len:15\n",
      "current self-play batch: 40\n",
      "num_playouts:200, win: 3, lose: 7, tie:0\n",
      "time cost is 154.06129002571106\n"
     ]
    }
   ],
   "source": [
    "start_t = time.time()\n",
    "training_pipeline = TrainPipeline()\n",
    "training_pipeline.run()\n",
    "print(\"time cost is {}\".format(time.time()-start_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f0464-331e-4c34-a504-23a70a814541",
   "metadata": {},
   "source": [
    "# AI battle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfb4cd34-600f-4b41-902d-bde6f4f49da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义当前玩家\n",
    "class CurPlayer:\n",
    "    player_id = 0\n",
    "\n",
    "\n",
    "# 可视化部分\n",
    "class Game(object):\n",
    "    def __init__(self, board):\n",
    "        self.board = board\n",
    "        self.cell_size = board_width - 1\n",
    "        self.chess_size = 50 * self.cell_size\n",
    "\n",
    "        self.whitex = []\n",
    "        self.whitey = []\n",
    "        self.blackx = []\n",
    "        self.blacky = []\n",
    "\n",
    "        # 棋盘背景色\n",
    "        self.color = \"#e4ce9f\"\n",
    "        self.colors = [[self.color] * self.cell_size for _ in range(self.cell_size)]\n",
    "\n",
    "    def graphic(self, board, player1, player2):\n",
    "        \"\"\"Draw the board and show game info\"\"\"\n",
    "        plt_fig, ax = plt.subplots(facecolor=self.color)\n",
    "        ax.set_facecolor(self.color)\n",
    "\n",
    "        # 制作棋盘\n",
    "        # mytable = ax.table(cellColours=self.colors, loc='center')\n",
    "        mytable = plt.table(cellColours=self.colors,\n",
    "                            colWidths=[1 / board_width] * self.cell_size,\n",
    "                            loc='center'\n",
    "                            )\n",
    "\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "        # 网格大小\n",
    "        cell_height = 1 / board_width\n",
    "        for pos, cell in mytable.get_celld().items():\n",
    "            cell.set_height(cell_height)\n",
    "\n",
    "        mytable.auto_set_font_size(False)\n",
    "        mytable.set_fontsize(self.cell_size)\n",
    "        ax.set_xlim([1, board_width * 2 + 1])\n",
    "        ax.set_ylim([board_height * 2 + 1, 1])\n",
    "        plt.title(\"Gomoku\")\n",
    "\n",
    "        plt.axis('off')\n",
    "        cur_player = CurPlayer()\n",
    "\n",
    "        while True:\n",
    "            # left down of mouse\n",
    "            try:\n",
    "                if cur_player.player_id == 1:\n",
    "                    move = player1.get_action(self.board)\n",
    "                    self.board.step(move)\n",
    "                    x, y = self.board.move_to_location(move)\n",
    "                    plt.scatter((y + 1) * 2, (x + 1) * 2, s=self.chess_size, c='white')\n",
    "                    cur_player.player_id = 0\n",
    "                elif cur_player.player_id == 0:\n",
    "                    move = player2.get_action(self.board)\n",
    "                    self.board.step(move)\n",
    "                    x, y = self.board.move_to_location(move)\n",
    "                    plt.scatter((y + 1) * 2, (x + 1) * 2, s=self.chess_size, c='black')\n",
    "                    cur_player.player_id = 1\n",
    "\n",
    "                end, winner = self.board.game_end()\n",
    "                if end:\n",
    "                    if winner != -1:\n",
    "                        ax.text(x=board_width, y=(board_height + 1) * 2 + 0.1,\n",
    "                                s=\"Game end. Winner is player {}\".format(cur_player.player_id), fontsize=10,\n",
    "                                color='red', weight='bold',\n",
    "                                horizontalalignment='center')\n",
    "                    else:\n",
    "                        ax.text(x=board_width, y=(board_height + 1) * 2 + 0.1,\n",
    "                                s=\"Game end. Tie Round\".format(cur_player.player_id), fontsize=10, color='red',\n",
    "                                weight='bold',\n",
    "                                horizontalalignment='center')                   \n",
    "                    return winner\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def start_play(self, player1, player2, start_player=0):\n",
    "        \"\"\"start a game between two players\"\"\"\n",
    "        if start_player not in (0, 1):\n",
    "            raise Exception('start_player should be either 0 (player1 first) '\n",
    "                            'or 1 (player2 first)')\n",
    "        self.board.reset()\n",
    "        p1, p2 = self.board.players\n",
    "        player1.set_player_ind(p1)\n",
    "        player2.set_player_ind(p2)\n",
    "\n",
    "        self.graphic(self.board, player1, player2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a408d450-b727-4c8e-8673-3e5c5e841341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 初始化棋盘\n",
    "board = GomokuEnv()\n",
    "game = Game(board)\n",
    "# 加载模型\n",
    "best_policy = PolicyValueNet(model_file=\"best_model.pt\")\n",
    "# 两个AI对打\n",
    "mcts_player = MCTSPlayer(best_policy.policy_value_fn)\n",
    "# 开始对打\n",
    "game.start_play(mcts_player, mcts_player, start_player=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f738d9-2360-42c6-b7e4-d4c1d7d66242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
